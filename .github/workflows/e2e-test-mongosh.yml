name: E2E Test - DocumentDB with mongosh

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch:
    inputs:
      documentdb_version:
        description: 'DocumentDB image version to test'
        required: false
        default: '16'
      node_count:
        description: 'Number of DocumentDB nodes'
        required: false
        default: '1'
      test_level:
        description: 'Test level to run'
        required: false
        default: 'full'
        type: choice
        options:
          - quick
          - integration
          - full

permissions:
  packages: write
  contents: read
  id-token: write
  actions: read
  attestations: write

env:
  CERT_MANAGER_NS: cert-manager
  OPERATOR_NS: documentdb-operator
  DB_NS: documentdb-e2e-test
  DB_NAME: documentdb-e2e
  DB_USERNAME: default_user
  DB_PASSWORD: Admin100
  DB_PORT: 10260

jobs:
  # Use the reusable build workflow
  build:
    name: Build Images and Charts
    uses: ./.github/workflows/build-and-package.yml
    with:
      image_tag_prefix: 'e2e-test'
      chart_version_prefix: '0.1.0'
      push_to_registry: true
    secrets: inherit

  e2e-test:
    name: Run E2E Tests
    runs-on: ${{ matrix.runner }}
    timeout-minutes: 60
    needs: build
    
    strategy:
      matrix:
        include:
          - architecture: amd64
            runner: ubuntu-latest
          # - architecture: arm64
          #   runner: ubuntu-22.04-arm
        # Test different scenarios
        test_scenario:
          - name: "single-node"
            node_count: 1
            instances_per_node: 1
    
    env:
      # Use outputs from the build workflow
      IMAGE_NAME: documentdb-kubernetes-operator
      IMAGE_TAG: ${{ needs.build.outputs.image_tag }}
      CHART_VERSION: ${{ needs.build.outputs.chart_version }}
      ARCHITECTURE: ${{ matrix.architecture }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4

    - name: Verify built image exists
      run: |
        echo "Verifying that our newly built image exists..."
        echo "Expected image: ghcr.io/${{ github.repository_owner }}/${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }}"
        
        # Login to GHCR to check image
        echo "${{ secrets.GITHUB_TOKEN }}" | docker login ghcr.io -u ${{ github.actor }} --password-stdin
        
        # Try to pull the image to verify it exists
        docker pull ghcr.io/${{ github.repository_owner }}/${{ env.IMAGE_NAME }}:${{ env.IMAGE_TAG }}
        echo "✓ Image verified successfully"

    - name: Setup dependencies
      uses: ./.github/actions/setup-dependencies
      with:
        architecture: ${{ matrix.architecture }}
        python-version: '3.11'
        helm-version: 'latest'

    - name: Setup K8s cluster
      uses: ./.github/actions/setup-k8s-cluster
      with:
        cluster-name: documentdb-e2e-${{ matrix.architecture }}-${{ matrix.test_scenario.name }}
        cert-manager-namespace: ${{ env.CERT_MANAGER_NS }}
        operator-namespace: ${{ env.OPERATOR_NS }}
        chart-version: ${{ env.CHART_VERSION }}
        github-token: ${{ secrets.GITHUB_TOKEN }}
        github-actor: ${{ github.actor }}
        github-repository-owner: ${{ github.repository_owner }}
        cert-manager-timeout: '10m'
        operator-timeout: '15m'
        architecture: ${{ matrix.architecture }}

    - name: Deploy DocumentDB Cluster - ${{ matrix.test_scenario.name }}
      uses: ./.github/actions/deploy-documentdb
      with:
        namespace: ${{ env.DB_NS }}
        cluster-name: ${{ env.DB_NAME }}
        node-count: ${{ matrix.test_scenario.node_count }}
        instances-per-node: ${{ matrix.test_scenario.instances_per_node }}
        documentdb-image: 'ghcr.io/microsoft/documentdb/documentdb-local:${{ github.event.inputs.documentdb_version || 16 }}'
        pvc-size: '10Gi'
        public-load-balancer: 'false'
        timeout: '900'
        architecture: ${{ matrix.architecture }}

    - name: Debug cluster state before connection setup
      run: |
        echo "=== Debugging cluster state on ${{ matrix.architecture }} ==="
        echo "Checking DocumentDB resource status:"
        kubectl get documentdb $DB_NAME -n $DB_NS -o yaml || echo "DocumentDB resource not found"
        
        echo "Checking all pods in namespace:"
        kubectl get pods -n $DB_NS -o wide
        
        echo "Checking DocumentDB-specific pods:"
        kubectl get pods -n $DB_NS -l cnpg.io/cluster=$DB_NAME -o wide || echo "No CNPG cluster pods found"
        
        echo "Checking for ready pods:"
        kubectl get pods -n $DB_NS -l cnpg.io/cluster=$DB_NAME --field-selector=status.phase=Running || echo "No running pods found"
        
        echo "Checking services:"
        kubectl get services -n $DB_NS
        
        echo "Checking recent events:"
        kubectl get events -n $DB_NS --sort-by='.lastTimestamp' --field-selector type!=Normal | tail -10

    - name: Wait for DocumentDB to be fully ready
      run: |
        echo "Waiting for DocumentDB cluster to be fully operational on ${{ matrix.architecture }}..."
        
        # Wait for the main DocumentDB pod (not initdb pods)
        timeout 300 bash -c '
        while true; do
          # Look for the main cluster pod (should be named like documentdb-e2e-1, not documentdb-e2e-1-initdb-*)
          MAIN_POD=$(kubectl get pods -n ${{ env.DB_NS }} -l cnpg.io/cluster=${{ env.DB_NAME }},cnpg.io/instanceRole=primary --no-headers 2>/dev/null | head -1 | awk "{print \$1}")
          
          if [ -n "$MAIN_POD" ] && [[ ! "$MAIN_POD" =~ initdb ]]; then
            POD_STATUS=$(kubectl get pod $MAIN_POD -n ${{ env.DB_NS }} -o jsonpath="{.status.phase}" 2>/dev/null)
            POD_READY=$(kubectl get pod $MAIN_POD -n ${{ env.DB_NS }} -o jsonpath="{.status.conditions[?(@.type==\"Ready\")].status}" 2>/dev/null)
            
            echo "Found main pod: $MAIN_POD, Status: $POD_STATUS, Ready: $POD_READY"
            
            if [ "$POD_STATUS" = "Running" ] && [ "$POD_READY" = "True" ]; then
              echo "✓ Main DocumentDB pod is ready: $MAIN_POD"
              break
            fi
          fi
          
          echo "Waiting for main DocumentDB pod to be ready..."
          kubectl get pods -n ${{ env.DB_NS }} -l cnpg.io/cluster=${{ env.DB_NAME }}
          sleep 10
        done
        ' || {
          echo "❌ Timeout waiting for DocumentDB pod to be ready"
          echo "Final pod status:"
          kubectl get pods -n ${{ env.DB_NS }} -o wide
          kubectl describe pods -n ${{ env.DB_NS }}
          exit 1
        }

    - name: Setup connection for comprehensive tests  
      uses: Wandalen/wretry.action@master
      id: comprehensive-connection
      with:
        attempt_limit: 3
        attempt_delay: 15000  # 15 seconds between attempts
        action: ./.github/actions/connection-test-setup
        with: |
          namespace: ${{ env.DB_NS }}
          cluster-name: ${{ env.DB_NAME }}
          port: ${{ env.DB_PORT }}
          architecture: ${{ matrix.architecture }}
          pid-file: '/tmp/comprehensive_pf_pid'
          connection-timeout: '120'
          wait-time: '30'

    - name: Verify port-forward stability and connectivity
      uses: Wandalen/wretry.action@master
      with:
        attempt_limit: 3
        attempt_delay: 20000  # 20 seconds between attempts
        pre_retry_command: |
          echo "Port-forward verification failed, checking status and restarting if needed..."
          
          # Check if port-forward process is still alive
          if [ -f '/tmp/comprehensive_pf_pid' ]; then
            PF_PID=$(cat /tmp/comprehensive_pf_pid 2>/dev/null || echo "unknown")
            if [ "$PF_PID" != "unknown" ]; then
              if ps -p $PF_PID > /dev/null; then
                echo "Port-forward process $PF_PID is still running, but connection verification failed"
                echo "Killing existing port-forward and will retry..."
                kill $PF_PID 2>/dev/null || true
              else
                echo "Port-forward process $PF_PID has died"
              fi
            fi
          fi
          
          # Clean up and restart port-forward
          pkill -f "kubectl.*port-forward.*${{ env.DB_PORT }}" || true
          sleep 5
          
          # Restart port-forward
          POD_NAME="${{ fromJSON(steps.comprehensive-connection.outputs.outputs).pod-name }}"
          echo "Restarting port-forward to pod $POD_NAME..."
          kubectl port-forward pod/$POD_NAME ${{ env.DB_PORT }}:${{ env.DB_PORT }} -n ${{ env.DB_NS }} &
          NEW_PF_PID=$!
          echo $NEW_PF_PID > /tmp/comprehensive_pf_pid
          echo "Restarted port-forward with new PID: $NEW_PF_PID"
          sleep 30  # Give more time for connection to stabilize
        command: |
          echo "Verifying port-forward stability and connectivity on ${{ matrix.architecture }}..."
          POD_NAME="${{ fromJSON(steps.comprehensive-connection.outputs.outputs).pod-name }}"
          echo "Testing connection to pod: $POD_NAME"
          
          # Extended stability test - check connection multiple times over a longer period
          echo "Running extended port-forward stability test..."
          
          for round in 1 2 3; do
            echo "=== Stability test round $round/3 ==="
            
            # Check if port-forward process is still running
            if [ -f '/tmp/comprehensive_pf_pid' ]; then
              PF_PID=$(cat /tmp/comprehensive_pf_pid 2>/dev/null || echo "unknown")
              if [ "$PF_PID" != "unknown" ]; then
                if ps -p $PF_PID > /dev/null; then
                  echo "✓ Port-forward process $PF_PID is running"
                else
                  echo "❌ Port-forward process $PF_PID has died in round $round"
                  exit 1
                fi
              fi
            fi
            
            # Test network connectivity multiple times
            CONNECTED=true
            for attempt in 1 2 3 4 5; do
              if timeout 10 bash -c 'nc -z 127.0.0.1 ${{ env.DB_PORT }}'; then
                echo "✓ Round $round, attempt $attempt: Port ${{ env.DB_PORT }} is accessible"
                sleep 2
              else
                echo "❌ Round $round, attempt $attempt: Cannot connect to port ${{ env.DB_PORT }}"
                CONNECTED=false
                break
              fi
            done
            
            if [ "$CONNECTED" = "false" ]; then
              echo "❌ Connection test failed in round $round"
              exit 1
            fi
            
            # Test actual mongosh connection
            echo "Testing mongosh connection in round $round..."
            if timeout 30 mongosh 127.0.0.1:${{ env.DB_PORT }} \
              -u ${{ env.DB_USERNAME }} \
              -p ${{ env.DB_PASSWORD }} \
              --authenticationMechanism SCRAM-SHA-256 \
              --tls \
              --tlsAllowInvalidCertificates \
              --eval "print('Stability test round $round successful')"; then
              echo "✓ Round $round: Mongosh connection successful"
            else
              echo "❌ Round $round: Mongosh connection failed"
              echo "Checking pod logs:"
              kubectl logs $POD_NAME -n ${{ env.DB_NS }} --tail=20 || echo "Failed to get logs"
              exit 1
            fi
            
            if [ $round -lt 3 ]; then
              echo "Waiting 10 seconds before next round..."
              sleep 10
            fi
          done
          
          echo "✓ Port-forward stability test completed successfully - connection is stable"

    - name: Execute comprehensive mongosh tests
      uses: Wandalen/wretry.action@master
      with:
        attempt_limit: 2
        attempt_delay: 20000  # 20 seconds between attempts 
        pre_retry_command: |
          echo "Retrying comprehensive tests..."
          # Quick connection check before retry
          if ! timeout 10 bash -c 'nc -z 127.0.0.1 ${{ env.DB_PORT }}'; then
            echo "Connection appears to be down, this may be a transient network issue"
            
            # Try to restart port-forward
            POD_NAME="${{ fromJSON(steps.comprehensive-connection.outputs.outputs).pod-name }}"
            pkill -f "kubectl.*port-forward.*${{ env.DB_PORT }}" || true
            sleep 5
            kubectl port-forward pod/$POD_NAME ${{ env.DB_PORT }}:${{ env.DB_PORT }} -n ${{ env.DB_NS }} &
            NEW_PF_PID=$!
            echo $NEW_PF_PID > /tmp/comprehensive_pf_pid
            echo "Restarted port-forward with PID: $NEW_PF_PID"
            sleep 20
          fi
        command: |
          echo "Running comprehensive mongosh validation tests on ${{ matrix.architecture }}..."
          echo "Using pod: ${{ fromJSON(steps.comprehensive-connection.outputs.outputs).pod-name }}"
          
          # Run comprehensive tests with validation using external script
          if mongosh 127.0.0.1:${{ env.DB_PORT }} \
            -u ${{ env.DB_USERNAME }} \
            -p ${{ env.DB_PASSWORD }} \
            --authenticationMechanism SCRAM-SHA-256 \
            --tls \
            --tlsAllowInvalidCertificates \
            --file scripts/test-scripts/comprehensive_mongosh_tests.js; then
            echo "✓ Comprehensive mongosh tests completed successfully on ${{ matrix.architecture }}"
          else
            echo "❌ Comprehensive mongosh tests failed on ${{ matrix.architecture }}"
            echo "Checking DocumentDB pod logs for errors:"
            POD_NAME="${{ fromJSON(steps.comprehensive-connection.outputs.outputs).pod-name }}"
            kubectl logs $POD_NAME -n ${{ env.DB_NS }} --tail=100 || echo "Failed to get logs"
            exit 1
          fi

    - name: Cleanup comprehensive test connection
      if: always()
      uses: ./.github/actions/cleanup-port-forward
      with:
        pid-file: '/tmp/comprehensive_pf_pid'
        architecture: ${{ matrix.architecture }}

    - name: Setup connection for performance tests
      uses: Wandalen/wretry.action@master
      id: performance-connection
      with:
        attempt_limit: 3
        attempt_delay: 15000  # 15 seconds between attempts
        action: ./.github/actions/connection-test-setup
        with: |
          namespace: ${{ env.DB_NS }}
          cluster-name: ${{ env.DB_NAME }}
          port: ${{ env.DB_PORT }}
          architecture: ${{ matrix.architecture }}
          pid-file: '/tmp/performance_pf_pid'
          connection-timeout: '120'
          wait-time: '30'

    - name: Verify performance test port-forward stability
      uses: Wandalen/wretry.action@master
      with:
        attempt_limit: 2
        attempt_delay: 15000
        pre_retry_command: |
          echo "Performance port-forward verification failed, restarting..."
          pkill -f "kubectl.*port-forward.*${{ env.DB_PORT }}" || true
          sleep 5
          POD_NAME="${{ fromJSON(steps.performance-connection.outputs.outputs).pod-name }}"
          kubectl port-forward pod/$POD_NAME ${{ env.DB_PORT }}:${{ env.DB_PORT }} -n ${{ env.DB_NS }} &
          NEW_PF_PID=$!
          echo $NEW_PF_PID > /tmp/performance_pf_pid
          sleep 20
        command: |
          echo "Verifying performance test connection stability..."
          POD_NAME="${{ fromJSON(steps.performance-connection.outputs.outputs).pod-name }}"
          
          # Quick stability check
          for i in 1 2 3; do
            if timeout 10 bash -c 'nc -z 127.0.0.1 ${{ env.DB_PORT }}'; then
              echo "✓ Connection check $i/3 passed"
              sleep 5
            else
              echo "❌ Connection check $i/3 failed"
              exit 1
            fi
          done
          
          # Test mongosh connection once
          if timeout 30 mongosh 127.0.0.1:${{ env.DB_PORT }} \
            -u ${{ env.DB_USERNAME }} \
            -p ${{ env.DB_PASSWORD }} \
            --authenticationMechanism SCRAM-SHA-256 \
            --tls \
            --tlsAllowInvalidCertificates \
            --eval "print('Performance test connection verified')"; then
            echo "✓ Performance test connection verified"
          else
            echo "❌ Performance test connection verification failed"
            exit 1
          fi

    - name: Execute performance tests
      uses: Wandalen/wretry.action@master
      with:
        attempt_limit: 2
        attempt_delay: 20000  # 20 seconds between attempts
        pre_retry_command: |
          echo "Retrying performance tests..."
          # Quick connection check and restart if needed
          if ! timeout 10 bash -c 'nc -z 127.0.0.1 ${{ env.DB_PORT }}'; then
            echo "Connection appears to be down, restarting port-forward..."
            POD_NAME="${{ fromJSON(steps.performance-connection.outputs.outputs).pod-name }}"
            pkill -f "kubectl.*port-forward.*${{ env.DB_PORT }}" || true
            sleep 5
            kubectl port-forward pod/$POD_NAME ${{ env.DB_PORT }}:${{ env.DB_PORT }} -n ${{ env.DB_NS }} &
            NEW_PF_PID=$!
            echo $NEW_PF_PID > /tmp/performance_pf_pid
            sleep 20
          fi
        command: |
          echo "Running performance validation tests on ${{ matrix.architecture }}..."
          echo "Using pod: ${{ fromJSON(steps.performance-connection.outputs.outputs).pod-name }}"
          
          # Run performance tests using external script
          if mongosh 127.0.0.1:${{ env.DB_PORT }} \
            -u ${{ env.DB_USERNAME }} \
            -p ${{ env.DB_PASSWORD }} \
            --authenticationMechanism SCRAM-SHA-256 \
            --tls \
            --tlsAllowInvalidCertificates \
            --file scripts/test-scripts/performance_test.js; then
            echo "✓ Performance tests completed successfully on ${{ matrix.architecture }}"
          else
            echo "❌ Performance tests failed on ${{ matrix.architecture }}"
            echo "Checking DocumentDB pod logs for errors:"
            POD_NAME="${{ fromJSON(steps.performance-connection.outputs.outputs).pod-name }}"
            kubectl logs $POD_NAME -n ${{ env.DB_NS }} --tail=100 || echo "Failed to get logs"
            exit 1
          fi

    - name: Cleanup performance test connection
      if: always()
      uses: ./.github/actions/cleanup-port-forward
      with:
        pid-file: '/tmp/performance_pf_pid'
        architecture: ${{ matrix.architecture }}

    - name: Ensure all port-forwards are cleaned up
      if: always()
      run: |
        echo "Ensuring all port-forward processes are properly cleaned up..."
        
        # Kill any remaining port-forward processes on our port
        pkill -f "kubectl.*port-forward.*${{ env.DB_PORT }}" || true
        
        # Clean up any remaining PID files
        rm -f /tmp/comprehensive_pf_pid /tmp/performance_pf_pid || true
        
        # Show final port-forward status
        ps aux | grep kubectl | grep port-forward || echo "✓ No port-forward processes remaining"
        
        echo "✓ Port-forward cleanup completed"

    - name: Test cluster health and monitoring
      run: |
        echo "Testing cluster health and monitoring on ${{ matrix.architecture }}..."
        
        # Check DocumentDB resource status
        kubectl get documentdb $DB_NAME -n $DB_NS -o yaml
        
        # Check pod resources and health
        kubectl top pods -n $DB_NS --containers || echo "Metrics server not available"
        
        # Check logs for any errors
        kubectl logs -n $DB_NS -l cnpg.io/cluster=$DB_NAME --tail=50
        
        # Check events
        kubectl get events -n $DB_NS --sort-by='.lastTimestamp'

    - name: Collect comprehensive logs on failure
      if: failure()
      uses: ./.github/actions/failure-diagnostics
      with:
        namespace: ${{ env.DB_NS }}
        cluster-name: ${{ env.DB_NAME }}
        operator-namespace: ${{ env.OPERATOR_NS }}
        architecture: ${{ matrix.architecture }}
        include-logs: 'true'
        log-lines: '200'
